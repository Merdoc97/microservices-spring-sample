apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: infra
data:
  fluent.conf: |
    <match fluent.**>
        # this tells fluentd to not output its log on stdout
        @type null
    </match>
    # here we read the logs from Docker's containers and parse them
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/app.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    <filter kubernetes.**>
         @type kubernetes_metadata
         format json
         #time_format %Y-%m-%dT%H:%M:%SZ
         #key_name log
         #reserve_data true
     </filter>
    # we use kubernetes metadata plugin to add metadatas to the log
     # we send the logs to Elasticsearch
    <match kubernetes.** >
           @type elasticsearch_dynamic
           include_tag_key true
           host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
           port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"

           scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"

           reload_connections true
           logstash_format true
           #Below line is use to isolate the indexes as per different namespaces in K8s.
           logstash_prefix kubernetes-${record['kubernetes']['namespace_name']}
           #Uncomment the below line, if want to isolate the indexes as per different pods in K8s.
           #logstash_prefix kubernetes-${record['kubernetes']['pod_name']}
           #<buffer>
               #@type file
               #path /var/log/fluentd-buffers/kubernetes.system.buffer
               #flush_mode interval
               #retry_type exponential_backoff
               #flush_thread_count 2
               #flush_interval 5s
               #retry_forever true
               #retry_max_interval 30
               #chunk_limit_size 2M
               #queue_limit_length 32
               #overflow_action block
           #</buffer>
        </match>